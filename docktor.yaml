version: "1"
service: web
compose_file: examples/docker-compose.yaml
scaling:
    cpu_high: 75
    cpu_low: 20
    min_replicas: 2
    max_replicas: 10
    scale_up_by: 2
    scale_down_by: 1
    check_interval: 10
    metrics_window: 10
llm:
    provider: dmr
    base_url: http://localhost:12434/engines/llama.cpp/v1
    model: ai/llama3.2
